<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="序NLP任务：分词、词性标注、未登录词识别 语言的性质：共时性；历时性 语法单位 1234句子是语言中最大的语法单位词组是词的组合，它是句子里面作用相当于词而本身又是由词组成的大于词的单位。词是最重要的一级语法单位，它是造句的时候能够独立运用的最小单位。语素是语言中音义结合的最小单位。就汉语来说，大抵一个汉字就是一个语素，但是也有两个字表示一个语素的，如：“咖啡” 语料库• 语料库（corpus）">
<meta property="og:type" content="article">
<meta property="og:title" content="NLP">
<meta property="og:url" content="http://example.com/2021/12/24/NLP/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="序NLP任务：分词、词性标注、未登录词识别 语言的性质：共时性；历时性 语法单位 1234句子是语言中最大的语法单位词组是词的组合，它是句子里面作用相当于词而本身又是由词组成的大于词的单位。词是最重要的一级语法单位，它是造句的时候能够独立运用的最小单位。语素是语言中音义结合的最小单位。就汉语来说，大抵一个汉字就是一个语素，但是也有两个字表示一个语素的，如：“咖啡” 语料库• 语料库（corpus）">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2021/12/24/NLP/image-20211227150000774.png">
<meta property="og:image" content="http://example.com/2021/12/24/NLP/image-20211227150045052.png">
<meta property="og:image" content="http://example.com/2021/12/24/NLP/image-20211227193857156.png">
<meta property="og:image" content="http://example.com/2021/12/24/NLP/image-20211227195204331.png">
<meta property="og:image" content="http://example.com/2021/12/24/NLP/image-20211227200001707.png">
<meta property="og:image" content="http://example.com/2021/12/24/NLP/image-20211227200011651.png">
<meta property="og:image" content="http://example.com/2021/12/24/NLP/image-20211227200200077.png">
<meta property="og:image" content="http://example.com/2021/12/24/NLP/image-20211227203707059.png">
<meta property="og:image" content="http://example.com/2021/12/24/NLP/image-20211227203903492.png">
<meta property="og:image" content="http://example.com/2021/12/24/NLP/image-20211227204121800.png">
<meta property="og:image" content="http://example.com/2021/12/24/NLP/image-20211227210645264.png">
<meta property="og:image" content="http://example.com/2021/12/24/NLP/image-20211227211103464.png">
<meta property="og:image" content="http://example.com/2021/12/24/NLP/image-20211227211441031.png">
<meta property="og:image" content="http://example.com/2021/12/24/NLP/image-20211227211917788.png">
<meta property="og:image" content="http://example.com/2021/12/24/NLP/image-20211227211938609.png">
<meta property="og:image" content="http://example.com/2021/12/24/NLP/image-20211227233833336.png">
<meta property="og:image" content="http://example.com/2021/12/24/NLP/image-20211227235729108.png">
<meta property="og:image" content="http://example.com/2021/12/24/NLP/image-20211228101907190.png">
<meta property="og:image" content="http://example.com/2021/12/24/NLP/image-20211228162546651.png">
<meta property="og:image" content="http://example.com/2021/12/24/NLP/image-20211228163639194.png">
<meta property="og:image" content="http://example.com/2021/12/24/NLP/image-20211228163649954.png">
<meta property="article:published_time" content="2021-12-24T09:08:09.000Z">
<meta property="article:modified_time" content="2021-12-28T16:31:44.720Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2021/12/24/NLP/image-20211227150000774.png">

<link rel="canonical" href="http://example.com/2021/12/24/NLP/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>NLP | Hexo</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hexo</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/12/24/NLP/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          NLP
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-12-24 17:08:09" itemprop="dateCreated datePublished" datetime="2021-12-24T17:08:09+08:00">2021-12-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-12-29 00:31:44" itemprop="dateModified" datetime="2021-12-29T00:31:44+08:00">2021-12-29</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="序"><a href="#序" class="headerlink" title="序"></a>序</h2><p>NLP任务：分词、词性标注、未登录词识别</p>
<p>语言的性质：共时性；历时性</p>
<p>语法单位</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">句子是语言中最大的语法单位</span><br><span class="line">词组是词的组合，它是句子里面作用相当于词而本身又是由词组成的大于词的单位。</span><br><span class="line">词是最重要的一级语法单位，它是造句的时候能够独立运用的最小单位。</span><br><span class="line">语素是语言中音义结合的最小单位。就汉语来说，大抵一个汉字就是一个语素，但是也有两个字表示一个语素的，如：“咖啡”</span><br></pre></td></tr></table></figure>
<h2 id="语料库"><a href="#语料库" class="headerlink" title="语料库"></a>语料库</h2><p>• 语料库（corpus）一词在语言学上意指大量的文本，通常经过整理， 具有既定格式与标记</p>
<p>语料库的种类</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">共时语料库与历时语料库</span><br><span class="line">通用语料库与专用语料库</span><br></pre></td></tr></table></figure>
<h3 id="语料加工"><a href="#语料加工" class="headerlink" title="语料加工"></a>语料加工</h3><p>文本处理</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">垃圾格式问题</span><br><span class="line">大小写</span><br><span class="line">标记化</span><br><span class="line">空格</span><br><span class="line">连字符</span><br><span class="line">词法</span><br><span class="line">句子定义—启发式算法</span><br><span class="line">句子边界的研究</span><br></pre></td></tr></table></figure>
<p>格式标注</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">通用标记语言SGML</span><br><span class="line">SGML是超文本格式的最高层次标准，是可以定义标记语言的元语言</span><br><span class="line">语法标注</span><br></pre></td></tr></table></figure>
<p>Zipf法则 • 一个词地频率f和它的词频排序位置r： f*r=k (k为常数)</p>
<p><img src="/2021/12/24/NLP/image-20211227150000774.png" alt="image-20211227150000774"></p>
<p>如果设置参数B=1, ρ=0，Mandelbrot公式就简化为Zipf法则</p>
<p>搭配抽取</p>
<p><img src="/2021/12/24/NLP/image-20211227150045052.png" alt="image-20211227150045052" style="zoom:50%;"></p>
<h2 id="语料库加工-双语句子自动对齐-amp-双语词典获取"><a href="#语料库加工-双语句子自动对齐-amp-双语词典获取" class="headerlink" title="语料库加工_双语句子自动对齐&amp; 双语词典获取"></a>语料库加工_双语句子自动对齐&amp; 双语词典获取</h2><h3 id="句子对齐问题描述"><a href="#句子对齐问题描述" class="headerlink" title="句子对齐问题描述"></a>句子对齐问题描述</h3><p>基于长度的句子对齐  基本思想：源语言和目标语言的句子长度存在一定 的比例关系</p>
<p>要求：最小（句珠内无句珠）； 唯一（一个句子仅属于一个句珠）； 无交叉（后句对齐一定在前句对齐位置之后）</p>
<h3 id="基于共现的双语词典的获取"><a href="#基于共现的双语词典的获取" class="headerlink" title="基于共现的双语词典的获取"></a>基于共现的双语词典的获取</h3><p>基本思想：如果汉语词出现在某个双语句对 中，其译文也必定在这个句对中。</p>
<p>汉英词典的迭代获取策略</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">迭代策略</span><br><span class="line">1) 初始化；</span><br><span class="line">2) 使用对数相似性模型计算汉英翻译词对候选；</span><br><span class="line">3) 选取前n个汉英对译词对；</span><br><span class="line">4) 双语句对中剔除选定的翻译词对；</span><br><span class="line">5) 若不满足终止条件，重复步骤2；</span><br><span class="line"> 几点说明：复合词暂未考虑；可加入交互方式;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>基于共现的词汇对译模型</p>
<p>评价方式：专家独立于上下文进行判别</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">评价1：每5000个翻译词对候选中正确的译文数</span><br><span class="line">评价2：综合考虑翻译词典的性能</span><br></pre></td></tr></table></figure>
<h2 id="汉语自动分词"><a href="#汉语自动分词" class="headerlink" title="汉语自动分词"></a>汉语自动分词</h2><h3 id="词法分析"><a href="#词法分析" class="headerlink" title="词法分析"></a>词法分析</h3><p>词干提取vs词形还原：分别用于IR 和 NLP</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">词干提取（stemming）是抽取词的词干或词根形式（不一定能够表达完整语义</span><br><span class="line">词形还原（lemmatization），是把一个任何形式的语言词汇还原为一般形式（能表达完整语义）</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">词干提取主要是采用“缩减”的方法</span><br><span class="line">词形还原主要采用“转变”的方法</span><br><span class="line">在复杂性上：词干提取方法相对简单，词形还原更复杂</span><br><span class="line">在实现方法上：主流方法类似，但具体实现上各有侧重</span><br></pre></td></tr></table></figure>
<p>词性标注</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">词性标注（part-of-speech tagging）,又称为词类标注或者简称</span><br><span class="line">标注，是指为分词结果中的每个单词标注一个正确的词性的程序，</span><br><span class="line">也即确定每个词是名词、动词、形容词或者其他词性的过程</span><br><span class="line">• 词性标注是很多NLP任务的预处理步骤，如句法分析，经过词性</span><br><span class="line">标注后的文本会带来很大的便利性，但也不是不可或缺的步骤</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="分词算法"><a href="#分词算法" class="headerlink" title="分词算法"></a>分词算法</h3><p>正向最大匹配分词(Forward Maximum  Matching method, FMM)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">基本思想：将当前能够匹配的最长词输出</span><br><span class="line">• 1. 设自动分词词典中最长词条所含汉字个数为I</span><br><span class="line">• 2. 取被处理材料当前字符串序数中的I个字作为匹配字段，查找分词词典。</span><br><span class="line">若词典中有这样的一个I字词，则匹配成功，匹配字段作为一个词被切分出</span><br><span class="line">来，转6</span><br><span class="line">• 3. 如果词典中找不到这样的一个I字词，则匹配失败</span><br><span class="line">• 4. 匹配字段去掉最后一个汉字，I--</span><br><span class="line">• 5. 重复2-4，直至切分成功为止</span><br><span class="line">• 6. I重新赋初值，转2，直到切分出所有词为止</span><br></pre></td></tr></table></figure>
<p>逆向最大匹配分词(Backward  Maximum Matching method, BMM法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">分词过程与FMM方法相同，不过是从句子(或文</span><br><span class="line">章)末尾开始处理，每次匹配不成功时去掉的是</span><br><span class="line">最前面的一个汉字</span><br></pre></td></tr></table></figure>
<p>实验表明：逆向最大匹配法比最大匹配法更有效</p>
<p>最大匹配法的问题</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">• 存在分词错误：增加知识、局部修改</span><br><span class="line">• 局部修改：增加歧义词表，排歧规则</span><br><span class="line">无法发现分词歧义-&gt;从单向匹配改为双向最大匹配</span><br></pre></td></tr></table></figure>
<p>双向匹配法（Bi-direction Matching  method, BM法）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">双向最大匹配法是将正向最大匹配法（FMM）得到的分词</span><br><span class="line">结果和逆向最大匹配法（BMM）得到的结果进行比较，从</span><br><span class="line">而决定正确的分词方法</span><br></pre></td></tr></table></figure>
<p>最少分词法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">• 分词结果中含词数最少</span><br><span class="line">• 优化代替了贪心</span><br><span class="line">• 等价于最短路径</span><br><span class="line">•算法：</span><br><span class="line">• 动态规划算法</span><br><span class="line">• 优点：好于单向的最大匹配方法</span><br><span class="line">• 最大匹配：独立自主/和平/等/互利/的/原则</span><br><span class="line">• 最短路径：独立自主/和/平等互利/的/原则</span><br><span class="line">• 缺点：忽略了所有覆盖歧义，也无法解决大部分交叉歧义</span><br><span class="line">• 结合成分子时</span><br><span class="line">• 结合|成分|子 &#123;&#125; 结|合成|分子 &#123;&#125; 结合|成|分</span><br></pre></td></tr></table></figure>
<h3 id="分词问题：歧义"><a href="#分词问题：歧义" class="headerlink" title="分词问题：歧义"></a>分词问题：歧义</h3><p>交集型切分歧义</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">汉字串AJB被称作交集型切分歧义，如果满足AJ、JB同时</span><br><span class="line">为词(A、J、B分别为汉字串)。此时汉字串J被称作交集串。</span><br></pre></td></tr></table></figure>
<p>组合型切分歧义</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">• 汉字串AB被称作组合型切分歧义，如果满足条件：A、</span><br><span class="line">B、AB同时为词</span><br></pre></td></tr></table></figure>
<p>交集型歧义字段中含有交集字段的个数，称为链长</p>
<p>“真歧义”和“伪歧义”</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">• 真歧义指存在两种或两种以上的可实现的切分形式</span><br><span class="line">• 伪歧义一般只有一种正确的切分形式</span><br></pre></td></tr></table></figure>
<p>分词问题</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">歧义</span><br><span class="line">未登录词</span><br><span class="line">新词</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>分词质量评价</p>
<p><img src="/2021/12/24/NLP/image-20211227193857156.png" alt="image-20211227193857156" style="zoom:50%;"></p>
<h2 id="中文分词-统计建模"><a href="#中文分词-统计建模" class="headerlink" title="中文分词_统计建模"></a>中文分词_统计建模</h2><h3 id="基于N元文法的分词（MM）"><a href="#基于N元文法的分词（MM）" class="headerlink" title="基于N元文法的分词（MM）"></a>基于N元文法的分词（MM）</h3><p>MM(马尔可夫模型/过程) ：有限历史假设，仅依 赖前n-1个词</p>
<p>一种最简化的情况：一元文法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">P（S）=p(w1) ·p(w2) ·p(w3)….p(wn)</span><br><span class="line"> 等价于最大频率分词</span><br><span class="line"> 即把切分路径上每一个词的概率相乘得到该切</span><br><span class="line">分路径的概率</span><br><span class="line"> 把词概率的负对数理解成路径“代价”，输出</span><br><span class="line">结果就是整体代价最“小”分词序列</span><br></pre></td></tr></table></figure>
<p>采用二元语法(bi-gram)：性能进一步提高</p>
<p><img src="/2021/12/24/NLP/image-20211227195204331.png" alt="image-20211227195204331" style="zoom:50%;"></p>
<p> 更大的n：对下一个词出现的约束性信息更多，更大的辨别力。  更小的n：出现的次数更多，更可靠的统计结果，更高的可靠性。</p>
<p>等价类映射：降低语言模型参数空间</p>
<p>数据平滑（smoothing）：保持模型的辨别能力</p>
<h3 id="基于HMM的分词-词性标注一体化"><a href="#基于HMM的分词-词性标注一体化" class="headerlink" title="基于HMM的分词/词性标注一体化"></a>基于HMM的分词/词性标注一体化</h3><p>输入：待处理句子S </p>
<p> 输出：S的 词序列 W = w1 ,w2…wn </p>
<p>词性序列 T = t1 ,t2…tn </p>
<p> 提示  W可以代表S  分词结果即观测序列  词性序列是状态序列</p>
<p>公式推导</p>
<p><img src="/2021/12/24/NLP/image-20211227200001707.png" alt="image-20211227200001707" style="zoom:50%;"></p>
<p><img src="/2021/12/24/NLP/image-20211227200011651.png" alt="image-20211227200011651" style="zoom:50%;"></p>
<p><img src="/2021/12/24/NLP/image-20211227200200077.png" alt="image-20211227200200077"></p>
<h3 id="由字构词的汉语分词方法"><a href="#由字构词的汉语分词方法" class="headerlink" title="由字构词的汉语分词方法"></a>由字构词的汉语分词方法</h3><p>基本思路  分词过程：一个字的分类问题；  每个字在词语中属于一个确定位置</p>
<p>字的的标注过程中，对所有的字根据预定义的特 征进行词位特征学习，获得一个概率模型</p>
<p>由字构词的分词技术的优势</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"> 简化了分词系统的设计  文本中的词表词和未登录词都是用统一的字 标注过程来实现的，分词过程成为字重组的 简单过程。  既可以不必专门强调词表词信息，也不用专 门设计特定的未登录词识别模块</span><br></pre></td></tr></table></figure>
<h3 id="汉语分词方法的后处理方法"><a href="#汉语分词方法的后处理方法" class="headerlink" title="汉语分词方法的后处理方法"></a>汉语分词方法的后处理方法</h3><p>为什么不采用更精巧的模型？</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">四元或更高阶...  不可行，需要大量的参数  不得不做一些平滑或差值  难度随模型复杂度而加剧</span><br></pre></td></tr></table></figure>
<p>两个重要组成部分：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"> 允许的错误校正转换的详细说明</span><br><span class="line"> 学习算法</span><br></pre></td></tr></table></figure>
<p>输入数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">一个已经标注好的语料库，</span><br><span class="line">*一个词典</span><br></pre></td></tr></table></figure>
<p>基于转换错误驱动的规则方法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"> 学习和标注在该方法种都是简单和直观的</span><br><span class="line"> 成功用于词性标注、句法分析、介词附着以及</span><br><span class="line">语义消歧</span><br><span class="line"> 经验上，没有出现过拟合现象</span><br><span class="line"> 可以被用来解决大部分后处理问题</span><br><span class="line"> 效率的提升优化，考验工程能力</span><br></pre></td></tr></table></figure>
<p>标注可以采用 </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"> 隐马尔科夫模型（HMM）  最大熵（ME）  最大熵马尔科夫模型（MEMM）  条件随机场（CRF）等</span><br></pre></td></tr></table></figure>
<h2 id="隐马尔科夫模型"><a href="#隐马尔科夫模型" class="headerlink" title="隐马尔科夫模型"></a>隐马尔科夫模型</h2><h3 id="马尔科夫-Markov-模型"><a href="#马尔科夫-Markov-模型" class="headerlink" title="马尔科夫(Markov)模型"></a>马尔科夫(Markov)模型</h3><p>马尔科夫模型是一种统计模型，广泛的应用在语音识别， 词性自动标注，音字转换，概率文法等各个自然语言处理 的应用领域。</p>
<p>随机过程又称为随机函数，是随时间随机变化的过程。马 尔科夫模型描述了一类重要随机过程。</p>
<p>系统在时间t处于状态𝑠𝑗的概率取决于其在时间1,2,…t-1的 状态，该概率为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">𝑃(𝑞𝑡 = 𝑠𝑗|𝑞𝑡−1 = 𝑠𝑖, 𝑞𝑡−2 = 𝑠𝑘, … )</span><br></pre></td></tr></table></figure>
<p>离散的一阶马尔科夫链：系统在时间t的状态只与时间t-1 的状态有关。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">𝑃(𝑞𝑡 = 𝑠𝑗|𝑞𝑡−1 = 𝑠𝑖, 𝑞𝑡−2 = 𝑠𝑘, … ) = 𝑃(𝑞𝑡 = 𝑠𝑗|𝑞𝑡−1 = 𝑠𝑖)</span><br></pre></td></tr></table></figure>
<p>状态转移概率𝑎𝑖𝑗必须满足以下条件：</p>
<p><img src="/2021/12/24/NLP/image-20211227203707059.png" alt="image-20211227203707059" style="zoom:50%;"></p>
<p>N个状态的一阶马尔科夫过程有𝑁2，可以表示成为一个状 态转移矩阵</p>
<p>eg:状态𝑠1：名词 状态𝑠2：动词 状态𝑠3：形容词</p>
<p>如果在该文字中某句子的第一个词为名词，那么该句子 中三类词出现顺序为O=“名动形名”的概率。</p>
<p><img src="/2021/12/24/NLP/image-20211227203903492.png" alt="image-20211227203903492" style="zoom:50%;"></p>
<p>马尔科夫(Markov)模型：有限状态机</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">马尔科夫模型可视为随机的有限状态机。</span><br><span class="line">圆圈表示状态，状态之间的转移用带箭头的弧表示，弧上</span><br><span class="line">的数字为状态转移的概率。</span><br><span class="line">初始状态用标记为start的输入箭头表示。</span><br><span class="line">假设任何状态都可作为终止状态。</span><br><span class="line">对每个状态来说，发出弧上的概率和为1。</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>eg:</p>
<p><img src="/2021/12/24/NLP/image-20211227204121800.png" alt="image-20211227204121800" style="zoom:50%;"></p>
<p>一般地，一个HMM记为一个五元组μ＝（S，K， A，B，π），其中，S为状态的集合，K为输出符 号的集合，π，A和B分别是初始状态的概率分布、 状态转移概率和符号发射概率。为了简单，有时也将其记为三元组μ＝（A，B，π）</p>
<p>隐马尔可夫模型：三个基本问题</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1.估值问题：给定一个观察序列 O = 𝑂1𝑂2 … 𝑂𝑇 和模型μ＝(A，</span><br><span class="line">B，π)，如何快速地计算出给定模型μ情况下，观察序列O的</span><br><span class="line">概率，即𝑃 𝑂 𝜇 ?</span><br><span class="line">2.序列问题：给定一个观察序列 O = 𝑂1𝑂2 … 𝑂𝑇 和模型μ＝(A，</span><br><span class="line">B，π),如何快速有效的选择在一定意义下“最优”的状态序</span><br><span class="line">列 𝑄 = 𝑞1𝑞2 … 𝑞𝑇 ，使得该状态序列“最好的解释”观察序列？</span><br><span class="line">3.参数估计问题：给定一个观察序列O = 𝑂1𝑂2 … 𝑂𝑇，如何根</span><br><span class="line">据最大似然估计来求模型的参数值？即如何调节模型μ＝(A，</span><br><span class="line">B，π)的参数，使得𝑃 𝑂 𝜇 最大？</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="隐马尔可夫模型：求解观察序列的概率"><a href="#隐马尔可夫模型：求解观察序列的概率" class="headerlink" title="隐马尔可夫模型：求解观察序列的概率"></a>隐马尔可夫模型：求解观察序列的概率</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">给定观察序列O = 𝑂1𝑂2 … 𝑂𝑇和模型𝜇 =(𝐴, 𝐵, π)，快速的计算出给定模型𝜇情况下观察序列O的概率，即𝑃 （𝑂|𝜇） 。</span><br></pre></td></tr></table></figure>
<p><img src="/2021/12/24/NLP/image-20211227210645264.png" alt="image-20211227210645264" style="zoom:50%;"></p>
<p>隐马尔可夫模型：前向算法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">基本思想：定义前向变量𝛼𝑡(𝑖)，前向变量𝛼𝑡(𝑖)是在时间t，HMM输出了序列𝑂1𝑂2 … 𝑂𝑡 ，并且位于状态𝑠𝑖的概率。</span><br></pre></td></tr></table></figure>
<p><img src="/2021/12/24/NLP/image-20211227211103464.png" alt="image-20211227211103464" style="zoom:50%;"></p>
<p>前向算法总的复杂度为O(𝑁2𝑇)</p>
<p>隐马尔可夫模型：后向算法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">后向变量𝛽𝑡(𝑖)是在给定模型𝜇 = (𝐴, 𝐵, π)，并且在时间t状态为𝑠𝑖的条件下，HMM输出观察序列𝑂𝑡+1 … 𝑂𝑇的概率。</span><br></pre></td></tr></table></figure>
<p>与计算前向变量一样，可以用动态规划的算法计算后向变量。</p>
<p><img src="/2021/12/24/NLP/image-20211227211441031.png" alt="image-20211227211441031" style="zoom:50%;"></p>
<p>时间复杂度：O(𝑁2𝑇)</p>
<h4 id="序列问题"><a href="#序列问题" class="headerlink" title="序列问题"></a>序列问题</h4><p>隐马尔可夫模型：维特比算法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">维特比算法用于求解HMM中的第二个问题，给定一个观</span><br><span class="line">察序列O = 𝑂1𝑂2 … 𝑂𝑇和模型𝜇 = (𝐴, 𝐵, π)，如何快速有效</span><br><span class="line">的选择在一定意义下最优的状态序列𝑄 = 𝑞1𝑞2 … 𝑞𝑇，使得</span><br><span class="line">该状态序列“最好的解释”观察序列。</span><br></pre></td></tr></table></figure>
<p><img src="/2021/12/24/NLP/image-20211227211917788.png" alt="image-20211227211917788" style="zoom:50%;"></p>
<p><img src="/2021/12/24/NLP/image-20211227211938609.png" alt="image-20211227211938609" style="zoom:50%;"></p>
<p>存在问题</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">单独最优不一定整体最优</span><br></pre></td></tr></table></figure>
<h4 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h4><p>最 大似然估计</p>
<p>EM</p>
<h2 id="句法分析"><a href="#句法分析" class="headerlink" title="句法分析"></a>句法分析</h2><h3 id="句法分析概述"><a href="#句法分析概述" class="headerlink" title="句法分析概述"></a>句法分析概述</h3><p>基本任务：确定句子的句法结构或句子中词汇之间的依存关系。</p>
<p>定义：判断单词序列（一般为句子）判读其构成是否合乎 给定的语法(recognition)，如果是，则给出其（树）结构 (parsing)</p>
<p>描述一种语言可以有三种途径</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">穷举法：把语言中的所有句子都枚举出来。显然，这种方法只适合句子数目有限的语</span><br><span class="line">语法/文法描述：语言中的每个句子用严格定义的规则来构造，利用规则生成语言中合法的句子</span><br><span class="line">自动机法：通过对输入句子进行合法性检验，区别哪些是语言中的句子，哪些不是语言中的句子</span><br></pre></td></tr></table></figure>
<p>形式语法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">四元组 𝐺 = &#123;𝑁, Σ, 𝑃, 𝑆&#125;</span><br><span class="line">𝑁是非终结符(non-terminal symbol)的有限集合(有时也称变量集或句法种类集)</span><br><span class="line">Σ是终结符号(terminal symbol)的有限集合，𝑁 ∩ Σ = ∅</span><br><span class="line">𝑃是一组重写规则的有限集合：𝑃 = 𝛼 → 𝛽 ，其中𝛼, 𝛽是由V中元素构成的串，但是𝛼中至少应含一个非终结符</span><br><span class="line">𝑆 ∈ 𝑁称为句子符或初始符</span><br></pre></td></tr></table></figure>
<p>形式语法种类</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">正则文法</span><br><span class="line">上下文无关文法</span><br><span class="line">上下文相关文法</span><br><span class="line">无约束文法</span><br></pre></td></tr></table></figure>
<p>控制策略</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">自顶向下、自底向上</span><br><span class="line">移进-归约是自底向上语法分析的一种形式</span><br><span class="line"> 使用一个栈来保存文法符号，并用一个输入缓冲区来存放将要进行语</span><br><span class="line">法分析的其余符号</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>搜索策略</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">深搜广搜</span><br></pre></td></tr></table></figure>
<p>扫描策略</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">自左至右，自右至左</span><br></pre></td></tr></table></figure>
<p>移进-归约是自底向上语法分析的一种形式</p>
<p>CFG缺陷</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"> 对于一个中等长度的输入句子来说，要利用大覆盖度的语法规</span><br><span class="line">则分析出所有可能的句子结构是非常困难的，分析过程的复杂</span><br><span class="line">度往往使程序无法实现</span><br><span class="line"> 即使能分析出句子所有可能的结构，也难以在巨大的句法分析</span><br><span class="line">结果集中实现有效的消歧，并选择出最有可能的分析结果</span><br><span class="line"> 手工编写的规则一般带有一定的主观性，对于实际应用系统来</span><br><span class="line">说，往往难以覆盖大领域的所有复杂语言</span><br><span class="line"> 写规则本身是一件大工作量的复杂劳动，而且编写的规则对特</span><br><span class="line">定的领域有密切的相关性，不利于句法分析系统向其他领域移</span><br><span class="line">植</span><br></pre></td></tr></table></figure>
<h3 id="概率上下文无关文法-PCFG"><a href="#概率上下文无关文法-PCFG" class="headerlink" title="概率上下文无关文法(PCFG)"></a>概率上下文无关文法(PCFG)</h3><p>概率上下文无关文法就是一个为规则增添了概率的简单CFG， 指明了不同重写规则的可能性大小</p>
<p>在基于PCFG的句法分析模型中，假设满足以下三个条件：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">上下文无关性</span><br><span class="line">祖先无关性</span><br><span class="line">位置不变性</span><br></pre></td></tr></table></figure>
<p>剪枝策略：Beam search（集束搜索）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">一种启发式图搜索算法，为了减少搜索占用的时间和空间，在每一步深度扩展的时候，</span><br><span class="line">减掉一些质量比较差的节点，保留质量较高的一些节点</span><br><span class="line">优点是减少空间消耗，提高时间效率</span><br><span class="line">缺点是有可能存在潜在的最佳方案被丢弃，beam search算法是不完全的</span><br></pre></td></tr></table></figure>
<p>PCFG的优点</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">可利用概率减少分析过程的搜索空间</span><br><span class="line">可以利用概率对概率较小的子树剪枝，加快分析效</span><br><span class="line">率</span><br><span class="line">可以定量地比较两个语法的性能</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>PCFG的缺陷</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">结构相关性</span><br><span class="line">词汇相关性</span><br></pre></td></tr></table></figure>
<h2 id="词义消歧"><a href="#词义消歧" class="headerlink" title="词义消歧"></a>词义消歧</h2><p>word sense disambiguation     WSD</p>
<p>义位：语义系统中能独立存在的基本语义单位</p>
<p>WSD需要解决三个问题：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(1)如何判断一个词是不是多义词？ 如何表示一个多义词的不同意思？</span><br><span class="line">(2)对每个多义词，预先要有关于它的 各个不同义项的清晰的区分标准</span><br><span class="line">(3)对出现在具体语境中的每个多义词，为它确定一个合适的义项</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">基于机器词典的WSD</span><br><span class="line">基于义类词典的WSD</span><br><span class="line">基于语料库的WSD</span><br><span class="line">基于统计方法的WSD</span><br><span class="line">基于规则的WSD</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">总结：</span><br><span class="line">用词典资源进行词义排歧，是利用词典中对多义</span><br><span class="line">词的各个义项的描写，求多义词的释义跟其上下</span><br><span class="line">文环境词的释义之间的交集，判断词义的亲和程</span><br><span class="line">度，来确定词义；</span><br><span class="line">由于词典释义的概括性，这种方法应用于实际语</span><br><span class="line">料中多义词的排歧，效果不一定理想</span><br></pre></td></tr></table></figure>
<p>基于义类词典的WSD方法</p>
<p><img src="/2021/12/24/NLP/image-20211227233833336.png" alt="image-20211227233833336" style="zoom:50%;"></p>
<p>互信息：I（X；Y）反映的是在知道了Y的值 以后X的不确定性的减少量。</p>
<p>基于Bayes判别的WSD方法</p>
<p><img src="/2021/12/24/NLP/image-20211227235729108.png" alt="image-20211227235729108" style="zoom:50%;"></p>
<p>词义消歧——基于多分类器集成</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">总结</span><br><span class="line">还有很多问题需要探讨</span><br><span class="line">❖如何选用更有效的分类器</span><br><span class="line">❖单分类器的结果怎样更高效地集成</span><br><span class="line">❖如何在单分类器中选取更有效的特征</span><br><span class="line"> 集成学习的研究对自然语言处理中的其他任务</span><br></pre></td></tr></table></figure>
<h2 id="篇章"><a href="#篇章" class="headerlink" title="篇章"></a>篇章</h2><p>概念</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Anaphor:指代语</span><br><span class="line">Entity(referent):实体（指称对象）</span><br><span class="line">Reference:指称。用于指称实体的语言表示</span><br><span class="line">Antecedent:先行语。语篇中引入的一个相对明确的指称意义表述（如张三）；</span><br><span class="line">Coreference:共指（同指）。当两种表述均指称相同对象（实体）时，这两种表述具有共指关系</span><br></pre></td></tr></table></figure>
<p>六类指称表示</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"> Indefinite NPs（不定名词）: 一辆汽车</span><br><span class="line"> Definite NPs （有定名词）: 那个人</span><br><span class="line"> Pronouns （人称代词）: 它，他</span><br><span class="line"> Demonstratives （指示代词）: 这，那</span><br><span class="line"> One-anaphora （one指代）: one (in English)</span><br><span class="line"> Zero anaphora （0型指代）: 省略</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>指代一般包括两种情况</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">– 回指(Anaphora)：强调指代语与另一个表述之间的关</span><br><span class="line">系。指代语的指称对象通常不明确，需要确定其与先行</span><br><span class="line">语之间的关系来解释指代语的语义</span><br><span class="line">• 张先生走过来，给大家看他的新作品</span><br><span class="line">– 共指(coreference)：强调一个表述与另一个表述是否</span><br><span class="line">指向相同的实体，可以独立于上下文存在</span><br><span class="line">• 第44任美国总统 与奥巴马</span><br></pre></td></tr></table></figure>
<h3 id="衔接和连贯"><a href="#衔接和连贯" class="headerlink" title="衔接和连贯"></a>衔接和连贯</h3><p>以词汇表示的关联，通常称为“衔接(cohesion)，强调其构成成分</p>
<p>通过句子意义表示的关联称为连贯Coherence，强调整体上表达某种意义</p>
<h3 id="篇章表示和相似度计算"><a href="#篇章表示和相似度计算" class="headerlink" title="篇章表示和相似度计算"></a>篇章表示和相似度计算</h3><p>将文档表示为如下所示的向量： 𝑑𝑗 = (𝑤1,𝑗 , 𝑤2,𝑗 , 𝑤3,𝑗 , … , 𝑤𝑡,𝑗)  向量的每一维都对应于词表中的一个词。  如果某个词出现在了文档中，那它在向量中的值就非 零。  这个值有很多计算方法，我们使用词语在文档中出现 的次数表示。</p>
<h2 id="机器翻译"><a href="#机器翻译" class="headerlink" title="机器翻译"></a>机器翻译</h2><h3 id="传统机器翻译方法"><a href="#传统机器翻译方法" class="headerlink" title="传统机器翻译方法"></a>传统机器翻译方法</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">直接翻译法</span><br></pre></td></tr></table></figure>
<p>基于规则的翻译方法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">对源语言和目标语言均进行适当描述</span><br><span class="line">吧翻译机制与语法分开</span><br><span class="line">用规则描述语法的翻译方式</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">▪优点：</span><br><span class="line">▪ 可以较好地保持原文的结构，产生的译文结构与源文的结构关系密切</span><br><span class="line">▪ 尤其对于语言现象的或句法结构的明确的源语言语句具有较强的处理能力</span><br><span class="line">▪弱点：</span><br><span class="line">▪ 规则一般由人工编写，工作量大，主观性强，一致性难以保障</span><br><span class="line">▪ 不利于系统扩充，对非规范语言现 象缺乏相应的处理能力</span><br></pre></td></tr></table></figure>
<p>基于实例的翻译方法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">方法：输入语句-&gt;与事例相似度比较-&gt;翻译结果</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">▪ 方法优点</span><br><span class="line">▪ 不要求源语言句子必须符合语法规定;</span><br><span class="line">▪ 翻译机制一般不需要对源语言句子做深入分析;</span><br><span class="line">▪ 方法弱点</span><br><span class="line">▪ 两个不同的句子之间的相似性(包括结构相似性和语义相似性)往往难以把握</span><br><span class="line">▪ 在口语中，句子结构一般比较松散，成分冗余和成分省略都较严重;</span><br><span class="line">▪ 系统往往难以处理事例库中没有记录的陌生的语言现象；</span><br><span class="line">▪ 当事例库达到一定规模时，其事例检索的效率较低;</span><br></pre></td></tr></table></figure>
<h3 id="基于统计的机器翻译模型"><a href="#基于统计的机器翻译模型" class="headerlink" title="基于统计的机器翻译模型"></a>基于统计的机器翻译模型</h3><p>噪声信道模型</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">一种语言T 由于经过一个噪声信道而发生变形从而在信道的另一端呈现为另一种语言 S</span><br></pre></td></tr></table></figure>
<p>翻译问题可定义为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">▪ 如何根据观察到的 S，恢复最为可能的T 问题。</span><br></pre></td></tr></table></figure>
<p><img src="/2021/12/24/NLP/image-20211228101907190.png" alt="image-20211228101907190" style="zoom:50%;"></p>
<p>▪三个关键问题 ▪ (1)估计语言模型概率 p(T)； ▪ (2)估计翻译概率 p(S|T)； ▪ (3)快速有效地搜索T 使得 p(T)×p(S | T) 最大</p>
<h4 id="基于词的统计机器翻译模型"><a href="#基于词的统计机器翻译模型" class="headerlink" title="基于词的统计机器翻译模型"></a>基于词的统计机器翻译模型</h4><p>IBM模型1：词汇翻译（词对齐）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">▪ 基于词的统计翻译模型</span><br><span class="line">▪ 引入了词对齐的问题</span><br><span class="line">▪ 通过EM算法学习词对齐</span><br><span class="line">▪ 缺陷</span><br><span class="line">▪ 无法刻画翻译过程中重排序、添词、舍词等情况；</span><br><span class="line">▪ 例如：</span><br><span class="line">▪ Seldom do I go to work by bus.</span><br><span class="line">▪ 我很少乘公共汽车上班</span><br></pre></td></tr></table></figure>
<p>IBM模型2：增加绝对对齐模型</p>
<p>IBM模型3：引入繁衍率模型</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">前述模型存在的问题</span><br><span class="line">▪ 在随机选择对位关系的情况下，与目标语言句子中的单词t对应的源语言句子中的单</span><br><span class="line">词数目是一个随机变量；</span><br></pre></td></tr></table></figure>
<p>繁衍率</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">定义：与目标语言句子中的单词t对应的源语言句子中的单词数目的变量</span><br><span class="line">▪ 记做Фt，称该变量为单词t的繁衍能力或产出率(fertility)。一个具体的取值记做：Фt</span><br><span class="line">▪ 繁衍率刻画的是目标语言单词与源语言单词之间一对多的关系</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="基于短语的统计机器翻译模型"><a href="#基于短语的统计机器翻译模型" class="headerlink" title="基于短语的统计机器翻译模型"></a>基于短语的统计机器翻译模型</h4><p>基本思想</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">▪ 把训练语料库中所有对齐的短语及其翻译概率存储起来，作为一部带</span><br><span class="line">概率的短语词典</span><br><span class="line">▪ 这里所说的短语是任意连续的词串，不一定是一个独立的语言单位</span><br><span class="line">▪ 翻译的时候将输入的句子与短语词典进行匹配，选择最好的短语划分，</span><br><span class="line">将得到的短语译文重新排序，得到最优的译文.</span><br></pre></td></tr></table></figure>
<h3 id="系统融合"><a href="#系统融合" class="headerlink" title="系统融合"></a>系统融合</h3><p>几个相似的系统执行同一个任务时，可能有多个输出结果，系统融合将这些结果进行融 合，抽取其有用信息，归纳得到任务的最终输出结果。</p>
<p>目标：最终的输出比之前的输入结果都要好</p>
<p>句子级系统融合</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">两种技术</span><br><span class="line">最小贝叶斯风险解码；通用线性模型</span><br></pre></td></tr></table></figure>
<p>句子级系统融合方法不会产生新的翻译句子，而是在已有的翻 译句子中挑选出最好的一个</p>
<p>短语级系统融合 ▪ 利用多个翻译系统的输出结果，重新抽取短语翻译规则集合，并利用 新的短语翻译规则进行重新解码</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">基本思想：首先合并参与融合的所有系统的短语表，从中抽取</span><br><span class="line">一个新的源语言到目标语言的短语表，然后使用新的短语表和</span><br><span class="line">语言模型去重新解码源语言句子。</span><br></pre></td></tr></table></figure>
<p>词语级系统融合 ▪ 首先将多个翻译系统的译文输出进行词语对齐，构建一个混淆网络， 对混淆网络中的每个位置的候选词进行置信度估计， 最后进行混淆网 络解码</p>
<p>小结</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">句子级系统融合</span><br><span class="line">▪ 未生成新的翻译假设，有效的保护原来翻译假设中短语的连续性和句子词序，但</span><br><span class="line">是也没有吸收借鉴其他翻译假设中词或者短语层次的知识。</span><br><span class="line">▪ 短语级系统融合</span><br><span class="line">▪ 借鉴其他翻译系统的短语表知识，利用传统的基于短语的翻译引擎来重新解码源</span><br><span class="line">语言的句子。有效的保持短语的连续性和译文的局部词序。但是不能很好的利用</span><br><span class="line">非连续短语和句法知识来克服译文的远距离调序问题</span><br><span class="line">▪ 词语级系统融合</span><br><span class="line">▪ 从词的粒度重组了输出译文，充分利用了各个翻译假设的词汇级别的知识，取长</span><br><span class="line">补短。但是在混淆网络解码时，并不能保证新生成的翻译句子的词序一致性和短</span><br><span class="line">语连续性</span><br></pre></td></tr></table></figure>
<h2 id="应用：语言自动生成"><a href="#应用：语言自动生成" class="headerlink" title="应用：语言自动生成"></a>应用：语言自动生成</h2><h3 id="自然语言生成概述"><a href="#自然语言生成概述" class="headerlink" title="自然语言生成概述"></a>自然语言生成概述</h3><p>NLG生成模型</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">1. 马尔可夫链：通过当前单词可以预测句子中的下一个单</span><br><span class="line">词。</span><br><span class="line">缺点：无法探测当前单词与句子中其他单词的关系以及句</span><br><span class="line">子的结构，使得预测结果不够准确。</span><br><span class="line">2. 循环神经网络(RNN)：通过前馈网络传递序列的每个项目</span><br><span class="line">信息，并将模型的输出作为序列中下一项的输入，每个项</span><br><span class="line">目存储前面步骤中的信息。</span><br><span class="line">优点：能够捕捉输入数据的序列特征</span><br><span class="line">缺点：第一，RNN短期记忆无法生成连贯的长句子；第二，</span><br><span class="line">因为 RNN 不能并行计算，无法适应主流趋势。</span><br><span class="line">3. 长短期记忆网络(LSTM)，解决梯度消失问题，但难以并行化</span><br><span class="line">4. Seq2Seq，能够解决大部分序列不等长的问题</span><br><span class="line">5. Attention模型</span><br><span class="line">6. Transformer模型，能够在不考虑单词位置的情况</span><br><span class="line">下，直接捕捉句子中所有单词之间的关系</span><br><span class="line">7. ELMO模型</span><br><span class="line">8. BERT模型</span><br></pre></td></tr></table></figure>
<h3 id="数据到文本的生成"><a href="#数据到文本的生成" class="headerlink" title="数据到文本的生成"></a>数据到文本的生成</h3><p>以包含键值对的数据作为输入，旨在 自动生成流畅的、贴近事实的文本以描 述输入数据。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"> 信号分析模块(Siganl Analysis)</span><br><span class="line"> 数据阐释模块(Data Interpretation)</span><br><span class="line"> 文档规划模块(Document Planning)</span><br><span class="line"> 微规划与实现模块(Microplanning and Realisation)</span><br></pre></td></tr></table></figure>
<p>应用领域：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"> 天气预报领域的文本生成系统</span><br><span class="line"> 针对空气质量的文本生成系统</span><br><span class="line"> 针对财经数据的文本生成系统</span><br><span class="line"> 面向医疗诊断数据的文本生成系统</span><br><span class="line"> 基于体育数据生成文本摘要</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="文本到文本的生成"><a href="#文本到文本的生成" class="headerlink" title="文本到文本的生成"></a>文本到文本的生成</h3><p>对给定文本进行变换和处理从而获得新文本的技术</p>
<p>应用</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> 对联自动生成</span><br><span class="line"> 诗歌自动生成</span><br><span class="line"> 作文自动生成</span><br><span class="line"> 对话生成*---这个任务现阶段一般不作为NLG的研究分支来探讨</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="词和文档表示与相似度计算"><a href="#词和文档表示与相似度计算" class="headerlink" title="词和文档表示与相似度计算"></a>词和文档表示与相似度计算</h2><h3 id="词的表示"><a href="#词的表示" class="headerlink" title="词的表示"></a>词的表示</h3><p>独热表示</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">每个词对应一个向量，向量的维度等于词典的大小，向量中只有一个元素值为1，其余的元素均为0 ，值为1的元素对应的下标为该词在词典中的位置</span><br></pre></td></tr></table></figure>
<p>词频 -逆文档频率(TF -IDF)</p>
<p>词嵌入方法的问题</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">静态词向量</span><br><span class="line">词向量无法随语境变化</span><br><span class="line">不能处理一词多义</span><br><span class="line">多义词无法区分多个含义</span><br><span class="line">不能有效区分反义词</span><br><span class="line">反义词的上下文很相似</span><br></pre></td></tr></table></figure>
<p>词向量</p>
<p>skip-gram</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1. 将目标词和邻近的 </span><br><span class="line">语境词作为正面例子。</span><br><span class="line">2.随机抽取词库中的其他词 </span><br><span class="line">词库中的其他词，以获得负面样本。</span><br><span class="line">3. 使用逻辑回归来训练一个分类器，以区分这两种情况。</span><br><span class="line">区分这两种情况。</span><br><span class="line">4. 使用权重作为嵌入。</span><br></pre></td></tr></table></figure>
<h3 id="文档表示"><a href="#文档表示" class="headerlink" title="文档表示"></a>文档表示</h3><p><img src="/2021/12/24/NLP/image-20211228162546651.png" alt="image-20211228162546651" style="zoom:50%;"></p>
<h3 id="文本相似度计算"><a href="#文本相似度计算" class="headerlink" title="文本相似度计算"></a>文本相似度计算</h3><p>编辑距离，动态规划</p>
<h2 id="信息抽取"><a href="#信息抽取" class="headerlink" title="信息抽取"></a>信息抽取</h2><h3 id="信息抽取的定义、任务及发展"><a href="#信息抽取的定义、任务及发展" class="headerlink" title="信息抽取的定义、任务及发展"></a>信息抽取的定义、任务及发展</h3><p>信息抽取中的主要任务</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">命名实体识别：</span><br><span class="line">识别和分类文本中出现的“实体提及”</span><br><span class="line">实体链接：</span><br><span class="line">将“实体提及”链接到知识库中对应的实体</span><br><span class="line">关系抽取：</span><br><span class="line">找到句子中有关系的两个实体，并识别出他们之间的关系类型</span><br><span class="line">事件抽取：</span><br><span class="line">事件抽取就要是找到一个事件对应的元素。</span><br></pre></td></tr></table></figure>
<h3 id="命名实体识别"><a href="#命名实体识别" class="headerlink" title="命名实体识别"></a>命名实体识别</h3><p>挑战</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">种类繁多，命名方式灵活多样</span><br><span class="line">同一实体对应很多变体</span><br><span class="line">相同的词或者短语可以表示不同类别的实</span><br><span class="line">体</span><br><span class="line">存在嵌套</span><br><span class="line">细粒度</span><br><span class="line">语言不断进化，新的挑战不断出现</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>主要方法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">基于规则的方法 基于词典的方法 机器学习方法 ◼最大熵 ◼条件随机场 ◼深度学</span><br></pre></td></tr></table></figure>
<p>命名实体识别的评价</p>
<p><img src="/2021/12/24/NLP/image-20211228163639194.png" alt="image-20211228163639194" style="zoom:50%;"></p>
<p><img src="/2021/12/24/NLP/image-20211228163649954.png" alt="image-20211228163649954" style="zoom:50%;"></p>
<h3 id="实体链接"><a href="#实体链接" class="headerlink" title="实体链接"></a>实体链接</h3><p>将“实体提及”链接到知识库中对应的实体</p>
<h3 id="关系抽取"><a href="#关系抽取" class="headerlink" title="关系抽取"></a>关系抽取</h3><p>自动识别由一对实体和联系这对实体的关系构成的 相关三元组</p>
<p>预定义关系抽取</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">任务</span><br><span class="line">给定实体关系类别，给定语料，抽取目标关系对</span><br><span class="line">评测语料（MUC, ACE, KBP, SemEval）</span><br><span class="line">专家标注语料，语料质量高</span><br><span class="line">抽取的目标类别已经定义好</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>基于神经网络的关系抽取方法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">主要问题：如何设计合理的网络结构，从而捕捉更多的信息，进而更准确的完成关系的抽取</span><br><span class="line">网络结构：不同的网络结构捕捉文本中不同的信息</span><br></pre></td></tr></table></figure>
<p>开放域关系抽取</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">实体类别和关系类别不固定、数量大</span><br><span class="line">难点问题</span><br><span class="line"> 如何获取训练语料</span><br><span class="line"> 如何获取实体关系类别</span><br><span class="line"> 如何针对不同类型目标文本抽取关系</span><br><span class="line">需要研究新的抽取方法</span><br><span class="line"> 基于句法的方法</span><br><span class="line"> 基于知识监督的方法</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="深度学习简介"><a href="#深度学习简介" class="headerlink" title="深度学习简介"></a>深度学习简介</h2><p>常用的深度学习模型</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">激活函数</span><br><span class="line">深度神经网络（Deep Neural Network, DNN）</span><br><span class="line">卷积神经网络（Convolutional Neural Network,CNN)</span><br><span class="line">循环神经网络 (Recurrent Neural Network, RNN) </span><br><span class="line">注意力机制（Attention Mechanisms）</span><br></pre></td></tr></table></figure>
<p>pooling</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">目的：</span><br><span class="line">扩大视野：就如同先从近处看一张图片，然后离远一些再看同一张图片，有些细节就会被忽略。</span><br><span class="line">降维：在保留图片局部特征的前提下，使得图片更小，更易于计算。</span><br><span class="line">平移不变性，轻微扰动不会影响输出。</span><br><span class="line">维持同尺寸，便于后端处理。</span><br></pre></td></tr></table></figure>
<p>深度学习模型的应用</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">DBN的应用</span><br><span class="line">基于DBN的问答对挖掘</span><br><span class="line">CNN的应用</span><br><span class="line">关系分类</span><br><span class="line">句子分类</span><br><span class="line">LSTM-RNN的应用</span><br><span class="line">命名实体识别</span><br><span class="line"></span><br></pre></td></tr></table></figure>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/12/24/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="prev" title="深度学习">
      <i class="fa fa-chevron-left"></i> 深度学习
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/01/04/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/" rel="next" title="编译原理">
      编译原理 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BA%8F"><span class="nav-number">1.</span> <span class="nav-text">序</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%AD%E6%96%99%E5%BA%93"><span class="nav-number">2.</span> <span class="nav-text">语料库</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AF%AD%E6%96%99%E5%8A%A0%E5%B7%A5"><span class="nav-number">2.1.</span> <span class="nav-text">语料加工</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%AD%E6%96%99%E5%BA%93%E5%8A%A0%E5%B7%A5-%E5%8F%8C%E8%AF%AD%E5%8F%A5%E5%AD%90%E8%87%AA%E5%8A%A8%E5%AF%B9%E9%BD%90-amp-%E5%8F%8C%E8%AF%AD%E8%AF%8D%E5%85%B8%E8%8E%B7%E5%8F%96"><span class="nav-number">3.</span> <span class="nav-text">语料库加工_双语句子自动对齐&amp; 双语词典获取</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%A5%E5%AD%90%E5%AF%B9%E9%BD%90%E9%97%AE%E9%A2%98%E6%8F%8F%E8%BF%B0"><span class="nav-number">3.1.</span> <span class="nav-text">句子对齐问题描述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E5%85%B1%E7%8E%B0%E7%9A%84%E5%8F%8C%E8%AF%AD%E8%AF%8D%E5%85%B8%E7%9A%84%E8%8E%B7%E5%8F%96"><span class="nav-number">3.2.</span> <span class="nav-text">基于共现的双语词典的获取</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B1%89%E8%AF%AD%E8%87%AA%E5%8A%A8%E5%88%86%E8%AF%8D"><span class="nav-number">4.</span> <span class="nav-text">汉语自动分词</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AF%8D%E6%B3%95%E5%88%86%E6%9E%90"><span class="nav-number">4.1.</span> <span class="nav-text">词法分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95"><span class="nav-number">4.2.</span> <span class="nav-text">分词算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%86%E8%AF%8D%E9%97%AE%E9%A2%98%EF%BC%9A%E6%AD%A7%E4%B9%89"><span class="nav-number">4.3.</span> <span class="nav-text">分词问题：歧义</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D-%E7%BB%9F%E8%AE%A1%E5%BB%BA%E6%A8%A1"><span class="nav-number">5.</span> <span class="nav-text">中文分词_统计建模</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8EN%E5%85%83%E6%96%87%E6%B3%95%E7%9A%84%E5%88%86%E8%AF%8D%EF%BC%88MM%EF%BC%89"><span class="nav-number">5.1.</span> <span class="nav-text">基于N元文法的分词（MM）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8EHMM%E7%9A%84%E5%88%86%E8%AF%8D-%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8%E4%B8%80%E4%BD%93%E5%8C%96"><span class="nav-number">5.2.</span> <span class="nav-text">基于HMM的分词&#x2F;词性标注一体化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%94%B1%E5%AD%97%E6%9E%84%E8%AF%8D%E7%9A%84%E6%B1%89%E8%AF%AD%E5%88%86%E8%AF%8D%E6%96%B9%E6%B3%95"><span class="nav-number">5.3.</span> <span class="nav-text">由字构词的汉语分词方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B1%89%E8%AF%AD%E5%88%86%E8%AF%8D%E6%96%B9%E6%B3%95%E7%9A%84%E5%90%8E%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95"><span class="nav-number">5.4.</span> <span class="nav-text">汉语分词方法的后处理方法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9A%90%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E6%A8%A1%E5%9E%8B"><span class="nav-number">6.</span> <span class="nav-text">隐马尔科夫模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB-Markov-%E6%A8%A1%E5%9E%8B"><span class="nav-number">6.1.</span> <span class="nav-text">马尔科夫(Markov)模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%EF%BC%9A%E6%B1%82%E8%A7%A3%E8%A7%82%E5%AF%9F%E5%BA%8F%E5%88%97%E7%9A%84%E6%A6%82%E7%8E%87"><span class="nav-number">6.1.1.</span> <span class="nav-text">隐马尔可夫模型：求解观察序列的概率</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BA%8F%E5%88%97%E9%97%AE%E9%A2%98"><span class="nav-number">6.1.2.</span> <span class="nav-text">序列问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1"><span class="nav-number">6.1.3.</span> <span class="nav-text">参数估计</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90"><span class="nav-number">7.</span> <span class="nav-text">句法分析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90%E6%A6%82%E8%BF%B0"><span class="nav-number">7.1.</span> <span class="nav-text">句法分析概述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A6%82%E7%8E%87%E4%B8%8A%E4%B8%8B%E6%96%87%E6%97%A0%E5%85%B3%E6%96%87%E6%B3%95-PCFG"><span class="nav-number">7.2.</span> <span class="nav-text">概率上下文无关文法(PCFG)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%8D%E4%B9%89%E6%B6%88%E6%AD%A7"><span class="nav-number">8.</span> <span class="nav-text">词义消歧</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AF%87%E7%AB%A0"><span class="nav-number">9.</span> <span class="nav-text">篇章</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%A1%94%E6%8E%A5%E5%92%8C%E8%BF%9E%E8%B4%AF"><span class="nav-number">9.1.</span> <span class="nav-text">衔接和连贯</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AF%87%E7%AB%A0%E8%A1%A8%E7%A4%BA%E5%92%8C%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97"><span class="nav-number">9.2.</span> <span class="nav-text">篇章表示和相似度计算</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91"><span class="nav-number">10.</span> <span class="nav-text">机器翻译</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BC%A0%E7%BB%9F%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E6%96%B9%E6%B3%95"><span class="nav-number">10.1.</span> <span class="nav-text">传统机器翻译方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E7%BB%9F%E8%AE%A1%E7%9A%84%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E6%A8%A1%E5%9E%8B"><span class="nav-number">10.2.</span> <span class="nav-text">基于统计的机器翻译模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E8%AF%8D%E7%9A%84%E7%BB%9F%E8%AE%A1%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E6%A8%A1%E5%9E%8B"><span class="nav-number">10.2.1.</span> <span class="nav-text">基于词的统计机器翻译模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E7%9F%AD%E8%AF%AD%E7%9A%84%E7%BB%9F%E8%AE%A1%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E6%A8%A1%E5%9E%8B"><span class="nav-number">10.2.2.</span> <span class="nav-text">基于短语的统计机器翻译模型</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%B3%BB%E7%BB%9F%E8%9E%8D%E5%90%88"><span class="nav-number">10.3.</span> <span class="nav-text">系统融合</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BA%94%E7%94%A8%EF%BC%9A%E8%AF%AD%E8%A8%80%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90"><span class="nav-number">11.</span> <span class="nav-text">应用：语言自动生成</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E7%94%9F%E6%88%90%E6%A6%82%E8%BF%B0"><span class="nav-number">11.1.</span> <span class="nav-text">自然语言生成概述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%88%B0%E6%96%87%E6%9C%AC%E7%9A%84%E7%94%9F%E6%88%90"><span class="nav-number">11.2.</span> <span class="nav-text">数据到文本的生成</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%87%E6%9C%AC%E5%88%B0%E6%96%87%E6%9C%AC%E7%9A%84%E7%94%9F%E6%88%90"><span class="nav-number">11.3.</span> <span class="nav-text">文本到文本的生成</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%8D%E5%92%8C%E6%96%87%E6%A1%A3%E8%A1%A8%E7%A4%BA%E4%B8%8E%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97"><span class="nav-number">12.</span> <span class="nav-text">词和文档表示与相似度计算</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AF%8D%E7%9A%84%E8%A1%A8%E7%A4%BA"><span class="nav-number">12.1.</span> <span class="nav-text">词的表示</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%87%E6%A1%A3%E8%A1%A8%E7%A4%BA"><span class="nav-number">12.2.</span> <span class="nav-text">文档表示</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97"><span class="nav-number">12.3.</span> <span class="nav-text">文本相似度计算</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96"><span class="nav-number">13.</span> <span class="nav-text">信息抽取</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96%E7%9A%84%E5%AE%9A%E4%B9%89%E3%80%81%E4%BB%BB%E5%8A%A1%E5%8F%8A%E5%8F%91%E5%B1%95"><span class="nav-number">13.1.</span> <span class="nav-text">信息抽取的定义、任务及发展</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB"><span class="nav-number">13.2.</span> <span class="nav-text">命名实体识别</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E4%BD%93%E9%93%BE%E6%8E%A5"><span class="nav-number">13.3.</span> <span class="nav-text">实体链接</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96"><span class="nav-number">13.4.</span> <span class="nav-text">关系抽取</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B"><span class="nav-number">14.</span> <span class="nav-text">深度学习简介</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">42</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
